package temp;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class AverageDriver {
  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    if (otherArgs.length != 2) {
      System.err.println("Usage: AverageDriver <in> <out>");
      System.exit(2);
    }
    Job job = Job.getInstance(conf, "Average Temperature Per Year");
    job.setJarByClass(AverageDriver.class);

    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));

    job.setMapperClass(AverageMapper.class);
    job.setReducerClass(AverageReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
package temp;

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class AverageMapper
     extends Mapper<LongWritable, Text, Text, IntWritable> {

  private static final int MISSING = 9999;

  @Override
  protected void map(LongWritable key, Text value, Context context)
      throws IOException, InterruptedException {
    String line = value.toString();
    String year = line.substring(15, 19);

    int temp;
    if (line.charAt(87) == '+') {
      temp = Integer.parseInt(line.substring(88, 92));
    } else {
      temp = Integer.parseInt(line.substring(87, 92));
    }

    String q = line.substring(92, 93);
    if (temp != MISSING && q.matches("[01459]")) {
      context.write(new Text(year), new IntWritable(temp));
    }
  }
}
package temp;

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class AverageReducer
     extends Reducer<Text, IntWritable, Text, IntWritable> {

  @Override
  protected void reduce(Text key, Iterable<IntWritable> vals, Context ctx)
      throws IOException, InterruptedException {
    int sum = 0, count = 0;
    for (IntWritable v : vals) {
      sum += v.get();
      count++;
    }
    if (count > 0) {
      ctx.write(key, new IntWritable(sum / count));
    }
  }
}
package meanmax;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class MeanMaxDriver {
  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    if (otherArgs.length != 2) {
      System.err.println("Usage: MeanMaxDriver <in> <out>");
      System.exit(2);
    }
    Job job = Job.getInstance(conf, "Mean Max Temperature Per Month");
    job.setJarByClass(MeanMaxDriver.class);

    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));

    job.setMapperClass(MeanMaxMapper.class);
    job.setReducerClass(MeanMaxReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
package meanmax;

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MeanMaxMapper
     extends Mapper<LongWritable, Text, Text, IntWritable> {

  private static final int MISSING = 9999;

  @Override
  protected void map(LongWritable key, Text value, Context context)
      throws IOException, InterruptedException {
    String line = value.toString();
    String month = line.substring(19, 21);

    int temp;
    if (line.charAt(87) == '+') {
      temp = Integer.parseInt(line.substring(88, 92));
    } else {
      temp = Integer.parseInt(line.substring(87, 92));
    }

    String q = line.substring(92, 93);
    if (temp != MISSING && q.matches("[01459]")) {
      context.write(new Text(month), new IntWritable(temp));
    }
  }
}
package meanmax;

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class MeanMaxReducer
     extends Reducer<Text, IntWritable, Text, IntWritable> {

  @Override
  protected void reduce(Text key, Iterable<IntWritable> vals, Context ctx)
      throws IOException, InterruptedException {
    int monthMax = Integer.MIN_VALUE;
    int totalMaxSum = 0, countDays = 0;

    // We assume data is grouped by month; find maximum per day (3 readings/day),
    // sum those maxima, then average across days.
    int dailyCount = 0;
    for (IntWritable v : vals) {
      monthMax = Math.max(monthMax, v.get());
      dailyCount++;
      if (dailyCount == 3) {
        totalMaxSum += monthMax;
        countDays++;
        monthMax = Integer.MIN_VALUE;
        dailyCount = 0;
      }
    }
    if (countDays > 0) {
      ctx.write(key, new IntWritable(totalMaxSum / countDays));
    }
  }
}
2. Sample Input File
Create a small file sample-ncdc.txt with a few lines of NCDC-format records. For example:

pgsql
Copy
Edit
0043011990999991950051518004+68750+023550FM-12+0382... quality etc
0043011990999991950051519004+68900+023550FM-12+0382... 
0043011990999991950051520004+69000+023550FM-12+0382...
# (add more lines as needed)
Make sure each line follows the fixed-width format used by NCDC.

3. Compile & Package
bash
Copy
Edit
# 1. Compile
mkdir -p build
javac -classpath `hadoop classpath` -d build \
  src/temp/*.java src/meanmax/*.java

# 2. Create JAR
jar -cvf weather-jobs.jar -C build .
4. Run on Hadoop
bash
Copy
Edit
# 1. Put sample into HDFS
hdfs dfs -mkdir -p /user/$USER/ncdc-input
hdfs dfs -put sample-ncdc.txt /user/$USER/ncdc-input/

# 2. Run Average-per-Year
hadoop jar weather-jobs.jar temp.AverageDriver \
  /user/$USER/ncdc-input /user/$USER/ncdc-output-year

# 3. View results
hdfs dfs -cat /user/$USER/ncdc-output-year/part-*

# 4. Run Mean-Max-per-Month
hadoop jar weather-jobs.jar meanmax.MeanMaxDriver \
  /user/$USER/ncdc-input /user/$USER/ncdc-output-month

# 5. View results
hdfs dfs -cat /user/$USER/ncdc-output-month/part-*
